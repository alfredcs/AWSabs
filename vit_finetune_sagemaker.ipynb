{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredcs/AWSabs/blob/master/vit_finetune_sagemaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecu89QZ2K0x7"
      },
      "source": [
        "# Huggingface Sagemaker - Vision Transformer \n",
        "\n",
        "### Image Classification with the `google/vit` on `cifar10`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5szwHtU9K0yE"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Welcome to our end-to-end binary Image-Classification example. In this demo, we will use the Hugging Faces `transformers` and `datasets` library together with Amazon SageMaker to fine-tune a pre-trained vision transformers on image classification. \n",
        "\n",
        "The script and notebook is inspired by [NielsRogges](https://github.com/NielsRogge) example notebook of [Fine-tune the Vision Transformer on CIFAR-10](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb). Niels was also the contributor of the Vision Transformer into `transformers`.\n",
        "\n",
        "\n",
        "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5IvrzhKK0yD"
      },
      "source": [
        "1. [Introduction](#Introduction)  \n",
        "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
        "    1. [Installation](#Installation)  \n",
        "    3. [Permissions](#Permissions)\n",
        "3. [Processing](#Preprocessing)   \n",
        "    1. [convert features and transform images](#convert-features-and-transform-images)  \n",
        "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
        "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
        "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSn6T7lfK0yE"
      },
      "source": [
        "![Bildschirmfoto%202021-06-09%20um%2010.08.22.png](attachment:Bildschirmfoto%202021-06-09%20um%2010.08.22.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRwcbjU7K0yF"
      },
      "source": [
        "# Development Environment and Permissions \n",
        "\n",
        "\n",
        "_**Use at least a `t3.large` instance otherwise preprocessing will take ages.**_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myA8vtPdK0yF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if not already installed_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLlTLjHaK0yG"
      },
      "outputs": [],
      "source": [
        "!pip install \"sagemaker>=2.31.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qx8bD_2K0yH"
      },
      "source": [
        "## Permissions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wXU6_g0K0yI"
      },
      "source": [
        "_If you are going to use Sagemaker in a local environment, you need access to an IAM Role with the required permissions for Sagemaker. You can find out more about this [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwDOUNxaK0yI"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "\n",
        "sess = sagemaker.Session()\n",
        "# sagemaker session bucket -> used for uploading data, models and logs\n",
        "# sagemaker will automatically create this bucket if it not exists\n",
        "sagemaker_session_bucket=None\n",
        "if sagemaker_session_bucket is None and sess is not None:\n",
        "    # set to default bucket if a bucket name is not given\n",
        "    sagemaker_session_bucket = sess.default_bucket()\n",
        "\n",
        "role = sagemaker.get_execution_role()\n",
        "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
        "\n",
        "print(f\"sagemaker role arn: {role}\")\n",
        "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
        "print(f\"sagemaker session region: {sess.boto_region_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSJQjp_K0yJ"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "We are using the `datasets` library to download and preprocess the `fashion-mnist` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html) are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39di_cUqK0yK"
      },
      "source": [
        "_Note from Nils: \"that in the ViT paper, the best results were obtained when fine-tuning at a higher resolution. For this, one interpolates the pre-trained absolute position embeddings\"._\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbrW3-GeK0yK"
      },
      "source": [
        "## Convert Features and transform images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jhekn2zK0yL"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from random import randint\n",
        "\n",
        "# dataset used\n",
        "dataset_name = 'cifar10'\n",
        "\n",
        "# s3 key prefix for the data\n",
        "s3_prefix = 'samples/datasets/cifar10'\n",
        "\n",
        "# FeatureExtractor used in preprocessing\n",
        "model_name = 'google/vit-base-patch16-224-in21k'\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFV5XH36K0yL"
      },
      "source": [
        "We are downsampling dataset to make it faster to preprocess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJGehQC8K0yL"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "train_dataset, test_dataset = load_dataset(dataset_name, \n",
        "                                           split=['train[:5000]', 'test[:2000]'])\n",
        "\n",
        "# display random sample\n",
        "sample_image = np.array(train_dataset[randint(0, 2000)][\"img\"],dtype=\"uint8\")\n",
        "\n",
        "Image.fromarray(sample_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBHpJpG2K0yM"
      },
      "outputs": [],
      "source": [
        "from datasets import Features, Array3D\n",
        "\n",
        "# we need to extend the features \n",
        "features = Features({\n",
        "    **train_dataset.features,\n",
        "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
        "})\n",
        "\n",
        "# extractor helper function\n",
        "def preprocess_images(examples):\n",
        "    # get batch of images\n",
        "    images = examples['img']\n",
        "    # convert to list of NumPy arrays of shape (C, H, W)\n",
        "    images = [np.array(image, dtype=np.uint8) for image in images]\n",
        "    images = [np.moveaxis(image, source=-1, destination=0) for image in images]\n",
        "    # preprocess and add pixel_values\n",
        "    inputs = feature_extractor(images=images)\n",
        "    examples['pixel_values'] = inputs['pixel_values']\n",
        "\n",
        "    return examples\n",
        "\n",
        "# preprocess dataset\n",
        "train_dataset = train_dataset.map(preprocess_images, batched=True,features=features)\n",
        "test_dataset = test_dataset.map(preprocess_images, batched=True,features=features)\n",
        "\n",
        "# set to torch format for training\n",
        "train_dataset.set_format('torch', columns=['pixel_values', 'label'])\n",
        "test_dataset.set_format('torch', columns=['pixel_values', 'label'])\n",
        "\n",
        "# remove unused column\n",
        "train_dataet = train_dataset.remove_columns(\"img\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83btuNOFK0yM"
      },
      "source": [
        "## Uploading data to `sagemaker_session_bucket`\n",
        "\n",
        "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz7JoNJoK0yM"
      },
      "outputs": [],
      "source": [
        "import botocore\n",
        "from datasets.filesystems import S3FileSystem\n",
        "\n",
        "s3 = S3FileSystem()  \n",
        "\n",
        "# save train_dataset to s3\n",
        "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
        "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
        "\n",
        "# save test_dataset to s3\n",
        "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
        "test_dataset.save_to_disk(test_input_path,fs=s3)\n",
        "\n",
        "print(f\"train dataset is uploaded to {training_input_path}\")\n",
        "print(f\"test dataset is uploaded to {test_input_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73P4J4FaK0yN"
      },
      "source": [
        "num_train_epochs# Fine-tuning & starting Sagemaker Training Job\n",
        "\n",
        "In order to create a sagemaker training job we need a `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In an Estimator, we define which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
        "\n",
        "```python\n",
        "/opt/conda/bin/python train.py --num_train_epochs 1 --model_name google/vit-base-patch16-224-in21k --per_device_train_batch_size 16\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9M6Hs2QK0yN"
      },
      "source": [
        "## Creating an Estimator and start a training job\n",
        "\n",
        "We are defining the hyperparameter `use_auth_token` with our token from huggingface.co/settings to automatically upload our model to the Hugging Face Model Hub afterwards. \n",
        "The `train.py` makes us of the `.push_to_hub()` of the Trainer API to automatically upload model to hf.co/models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30R_aebDK0yN"
      },
      "outputs": [],
      "source": [
        "from sagemaker.huggingface import HuggingFace\n",
        "\n",
        "# hyperparameters, which are passed into the training job\n",
        "hyperparameters={'num_train_epochs': 3, # train epochs\n",
        "                 'per_device_train_batch_size': 16, # batch size\n",
        "                 'model_name': model_name, # model which will be trained on\n",
        "                 'use_auth_token': '' # add you API Token to upload the model\n",
        "                 }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XJB2TpdK0yN"
      },
      "outputs": [],
      "source": [
        "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
        "                            source_dir='./scripts',\n",
        "                            instance_type='ml.p3.2xlarge',\n",
        "                            instance_count=1,\n",
        "                            role=role,\n",
        "                            transformers_version='4.6',\n",
        "                            pytorch_version='1.7',\n",
        "                            py_version='py36',\n",
        "                            hyperparameters = hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dsFGZZYK0yO"
      },
      "outputs": [],
      "source": [
        "# starting the train job with our uploaded datasets as input\n",
        "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLIvhC0lK0yO"
      },
      "source": [
        "## Upload to hub\n",
        "\n",
        "Since we have done the preprocessing in advance we need to upload the `feature_extractor` separately. You can this by creating a `preprocessor_config.json` file in the UI on huggingface.co or using the `huggingface_hub` python library. \n",
        "\n",
        "![Bildschirmfoto%202021-06-09%20um%2010.02.52.png](attachment:Bildschirmfoto%202021-06-09%20um%2010.02.52.png)\n",
        "\n",
        "The file needs to contain the configuration of the `feature_extractor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Pju6lVK0yO",
        "outputId": "033606f9-20c1-483d-90df-b56ce1e70639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"do_normalize\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"size\": 224\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(feature_extractor.to_json_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ-KfruVK0yP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python385jvsc74a57bd01c5d1376e9e7e846f90d37bdf026c5f86f7cfa5de9097583bdee4c45d3da3d79"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of sagemaker-notebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}