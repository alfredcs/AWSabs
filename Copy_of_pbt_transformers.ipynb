{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of pbt_transformers.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredcs/AWSabs/blob/master/Copy_of_pbt_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE04qAVhTZNE"
      },
      "source": [
        "# Tuning ðŸ¤— Transformers with Population Based Training\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfFpbg76vVPy"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoS-HKJxtAZY"
      },
      "source": [
        "The first step is to import our main libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDe8FU6-swMI"
      },
      "source": [
        "In this notebook we show how to fine tune our Huggingface transformers using Population Based Training. The corresponding blog post is [here](https://medium.com/@amog_97444/c4e32c6c989b?source=friends_link&sk=92c2ed36420cd9e26281fd51da7c19b6).\n",
        "\n",
        "For our implementation of the fine tuning, we used [Ray Tune](https://https://docs.ray.io/en/master/tune/index.html), an open source library for scalable hyperparameter tuning. It is built on top of the [Ray](https://https://ray.io/) framework, which makes it perfect for parallel hyperparameter tuning on multiple GPUs. Make sure to set you runtime to use GPUs when going through this notebook. Since Colab provides us with limited memory and a single GPU, we use a much smaller transformer (tiny-distilroberta), run only 3 samples, and use a perturbation interval of 2 iterations in this notebook. The results in the blog post were obtained with a standard BERT model, 8 samples, perturbation after every iteration, and was run on a AWS p3.16xlarge instance. The exact code used for the blog post is [here](https://https://docs.ray.io/en/master/tune/examples/pbt_transformers.html)\n",
        "\n",
        "Letâ€™s take a look at how we can implement parallel Population Based Training for our transformers using this library!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l45TvyLw5si5"
      },
      "source": [
        "!pip install transformers==3.0.2\n",
        "!pip install ray==0.8.7\n",
        "!pip install ray[tune]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZA5KV9Ct1--"
      },
      "source": [
        "Depending on your current setup, there might be other libraries you have to install like torch. Also if youâ€™re wondering how I made the beautiful plots in the blog post, itâ€™s with a library called [Weights & Biases](https://https://www.wandb.com/). If you'd like, weâ€™ll go through how we can easily integrate W&B with our code as well so you can visualize your training runs, though using W&B is optional. First, create an account with them, and then we can install it and login:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqhhzz6YtRqD"
      },
      "source": [
        "!pip install wandb\n",
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"] = \"567cfcfcfb79b870512bc37972a2c7d1a3d158f8\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT2cmzH5ueS8"
      },
      "source": [
        "Now we can get started with our code! The first step is to start up ray. If youâ€™re running this on a cluster, make sure to specify an address to ray. For this notebook example, we don't have to worry about this. Also make sure to set log_to_driver to False, otherwise we get hit with a bunch of unnecessary tqdm training bars!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiAD-EtCvdtB"
      },
      "source": [
        "import ray\n",
        "\n",
        "# If running on a cluster uncomment use the line below instead \n",
        "# ray.init(address=\"auto\", log_to_driver=False)\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(log_to_driver=True, ignore_reinit_error=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT0mRl-dvsOA"
      },
      "source": [
        "Then, we can load and cache our transformer model, tokenizer, and the RTE dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcfbQ8FmuIM9"
      },
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# Util import\n",
        "from ray.tune.examples.pbt_transformers import utils\n",
        "\n",
        "\n",
        "\n",
        "# Set this to whatever you like\n",
        "data_dir_name = \"./data\"\n",
        "data_dir = os.path.abspath(os.path.join(os.getcwd(), data_dir_name))\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir, 0o755)\n",
        "\n",
        "# Change these as needed.\n",
        "model_name = \"sshleifer/tiny-distilroberta-base\"\n",
        "task_name = \"rte\"\n",
        "\n",
        "task_data_dir = os.path.join(data_dir, task_name.upper())\n",
        "\n",
        "# Download and cache tokenizer, model, and features\n",
        "print(\"Downloading and caching Tokenizer\")\n",
        "\n",
        "# Triggers tokenizer download to cache\n",
        "AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"Downloading and caching pre-trained model\")\n",
        "\n",
        "# Triggers model download to cache\n",
        "AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Download data.\n",
        "utils.download_data(task_name, data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB5BLmKev_6-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ICOgXTwEIh"
      },
      "source": [
        "With everything now downloaded and cached, we can now set up our training function. Our training function defines the training execution for a single hyperparameter configuration. For now we pull these hyperparameters from a config argument, but weâ€™ll see later how this is passed in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9GTvUzawE5b"
      },
      "source": [
        "First we get our datasets- we only use the first half of the dev dataset for validation, and leave the rest of testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uikm4YpavIOh"
      },
      "source": [
        "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
        "from transformers import GlueDataset\n",
        "\n",
        "def get_datasets(config):\n",
        "  data_args = DataTrainingArguments(\n",
        "        task_name=config[\"task_name\"], data_dir=config[\"data_dir\"])\n",
        "  tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
        "  train_dataset = GlueDataset(\n",
        "      data_args,\n",
        "      tokenizer=tokenizer,\n",
        "      mode=\"train\",\n",
        "      cache_dir=config[\"data_dir\"])\n",
        "  eval_dataset = GlueDataset(\n",
        "      data_args,\n",
        "      tokenizer=tokenizer,\n",
        "      mode=\"dev\",\n",
        "      cache_dir=config[\"data_dir\"])\n",
        "  # Only use the first half for validation\n",
        "  eval_dataset = eval_dataset[:len(eval_dataset) // 2]\n",
        "  return train_dataset, eval_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd7nAyxZ16W_"
      },
      "source": [
        "### Checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw14Ot7n1HQ-"
      },
      "source": [
        "We also need to add extra functionality for *checkpointing*. After every epoch of training, we need to save our training state. This is crucial for Population Based Training since it allows us to continue training from where we left off even when hyperparameters are perturbed. The Huggingface Trainer provides functionality to save and load from a checkpoint, but we do have to make some modifications to integrate this with Ray Tune checkpointing and to checkpoint after every epoch. The first step is to subclass the Trainer from the transformers library. Ray Tune provides this [TuneTransformerTrainer](https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/pbt_transformers/trainer.py) subclass which we utilize. Take a look at the class- we see that it handles reporting evaluation metrics to Tune, checkpointing everytime evaluate is called, and even a way to pass in custom W&B arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoXMZicduw3Q"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "from typing import Dict, Optional, Tuple\n",
        "\n",
        "from ray import tune\n",
        "\n",
        "import transformers\n",
        "from transformers.file_utils import is_torch_tpu_available\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, is_wandb_available\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "if is_wandb_available():\n",
        "  import wandb\n",
        "\n",
        "class TuneTransformerTrainer(transformers.Trainer):\n",
        "    def get_optimizers(\n",
        "            self, num_training_steps\n",
        "    ):\n",
        "        self.current_optimizer, self.current_scheduler = super(\n",
        "        ).get_optimizers(num_training_steps)\n",
        "        return (self.current_optimizer, self.current_scheduler)\n",
        "\n",
        "    def evaluate(self,\n",
        "                 eval_dataset= None):\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        output = self._prediction_loop(\n",
        "            eval_dataloader, description=\"Evaluation\")\n",
        "        self._log(output.metrics)\n",
        "\n",
        "        self.save_state()\n",
        "\n",
        "        tune.report(**output.metrics)\n",
        "\n",
        "        return output.metrics\n",
        "\n",
        "    def save_state(self):\n",
        "        with tune.checkpoint_dir(step=self.global_step) as checkpoint_dir:\n",
        "            self.args.output_dir = checkpoint_dir\n",
        "            # This is the directory name that Huggingface requires.\n",
        "            output_dir = os.path.join(\n",
        "                self.args.output_dir,\n",
        "                f\"{PREFIX_CHECKPOINT_DIR}-{self.global_step}\")\n",
        "            self.save_model(output_dir)\n",
        "            if self.is_world_master():\n",
        "                torch.save(self.current_optimizer.state_dict(),\n",
        "                           os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                torch.save(self.current_scheduler.state_dict(),\n",
        "                           os.path.join(output_dir, \"scheduler.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHG0h9qUFOv3"
      },
      "source": [
        "The only addition we have to make is to add a function to recover the checkpoint file from Tune's checkpoint directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wekJMP34FUK1"
      },
      "source": [
        "def recover_checkpoint(tune_checkpoint_dir, model_name=None):\n",
        "    if tune_checkpoint_dir is None or len(tune_checkpoint_dir) == 0:\n",
        "        return model_name\n",
        "    # Get subdirectory used for Huggingface.\n",
        "    subdirs = [\n",
        "        os.path.join(tune_checkpoint_dir, name)\n",
        "        for name in os.listdir(tune_checkpoint_dir)\n",
        "        if os.path.isdir(os.path.join(tune_checkpoint_dir, name))\n",
        "    ]\n",
        "    # There should only be 1 subdir.\n",
        "    assert len(subdirs) == 1, subdirs\n",
        "    return subdirs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuILNQN6xnxw"
      },
      "source": [
        "Finally, we put all of these together as well as create our training arguments, model, and Huggingface Trainer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOdga53vxVtx"
      },
      "source": [
        "from transformers import AutoConfig, TrainingArguments, glue_tasks_num_labels\n",
        "from ray.tune.integration.wandb import wandb_mixin\n",
        "\n",
        "@wandb_mixin\n",
        "def train_transformer(config, checkpoint_dir=None):\n",
        "  train_dataset, eval_dataset = get_datasets(config)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "        output_dir=tune.get_trial_dir(),\n",
        "        learning_rate=config[\"learning_rate\"],\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        evaluate_during_training=True,\n",
        "        # Run eval after every epoch.\n",
        "        eval_steps=(len(train_dataset) // config[\"per_gpu_train_batch_size\"]) +\n",
        "        1,\n",
        "        # We explicitly set save to 0, and do checkpointing in evaluate instead\n",
        "        save_steps=0,\n",
        "        num_train_epochs=config[\"num_epochs\"],\n",
        "        max_steps=config[\"max_steps\"],\n",
        "        per_device_train_batch_size=config[\"per_gpu_train_batch_size\"],\n",
        "        per_device_eval_batch_size=config[\"per_gpu_val_batch_size\"],\n",
        "        warmup_steps=0,\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "  model_name_or_path = recover_checkpoint(checkpoint_dir, config[\"model_name\"])\n",
        "  num_labels = glue_tasks_num_labels[config[\"task_name\"]]\n",
        "\n",
        "  config = AutoConfig.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=task_name,\n",
        "    )\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        config=config,\n",
        "    )\n",
        "   \n",
        "  # Use our modified TuneTransformerTrainer\n",
        "  tune_trainer = TuneTransformerTrainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=eval_dataset,\n",
        "      compute_metrics=utils.build_compute_metrics_fn(task_name),\n",
        "  )\n",
        "  tune_trainer.train(model_name_or_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EhcYLiZHLzB"
      },
      "source": [
        "Our training function takes in 2 parameters: config which contains all of our hyperparameters, and checkpoint_dir which is a directory containing the previous state of our trial. As we'll see below, these 2 arguments are passed in to our training function by Tune\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11xEMV8mHiJ4"
      },
      "source": [
        "## Hyperparameter Tuning with Ray Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYxmkIpuHlSx"
      },
      "source": [
        "Now that we have our training function setup, we run our hyperparameter search with Ray Tune. We first create an initial hyperparameter configuration which specifies the hyperparameters each trial will use initially. For some of our hyperparameters, we want to try different configurations, so we sample those from a distribution.\n",
        "\n",
        "We also pass in our W&B arguments here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqNbMF1CHf3H"
      },
      "source": [
        "config = {\n",
        "        # These 3 configs below were defined earlier\n",
        "        \"model_name\": model_name,\n",
        "        \"task_name\": task_name,\n",
        "        \"data_dir\": task_data_dir,\n",
        "        \"per_gpu_val_batch_size\": 32,\n",
        "        \"per_gpu_train_batch_size\": tune.choice([16, 32, 64]),\n",
        "        \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
        "        \"weight_decay\": tune.uniform(0.0, 0.3),\n",
        "        \"num_epochs\": tune.choice([2, 3, 4, 5]),\n",
        "        \"max_steps\": -1,  # We use num_epochs instead.\n",
        "        \"wandb\": {\n",
        "            \"project\": \"pbt_transformers\",\n",
        "            \"reinit\": True,\n",
        "            \"allow_val_change\": True\n",
        "        }\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGIax71xIDqy"
      },
      "source": [
        "Now we can set up our Population Based Training scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip6shHkNICTs"
      },
      "source": [
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "\n",
        "scheduler = PopulationBasedTraining(\n",
        "        time_attr=\"training_iteration\",\n",
        "        metric=\"eval_acc\",\n",
        "        mode=\"max\",\n",
        "        perturbation_interval=2,\n",
        "        hyperparam_mutations={\n",
        "            \"weight_decay\": lambda: tune.uniform(0.0, 0.3).func(None),\n",
        "            \"learning_rate\": lambda: tune.uniform(1e-5, 5e-5).func(None),\n",
        "            \"per_gpu_train_batch_size\": [16, 32, 64],\n",
        "        })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvmerUBvIMei"
      },
      "source": [
        "We also create a CLI reporter to view our results from the command line. We specify the hyperparameters we want to see from the command line, as well as what metrics we want to see. The metrics are the inputs to the tune.report we call we make in TuneTransformerTrainer.evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMi3NOkPIJS4"
      },
      "source": [
        "from ray.tune import CLIReporter\n",
        "\n",
        "reporter = CLIReporter(\n",
        "        parameter_columns={\n",
        "            \"weight_decay\": \"w_decay\",\n",
        "            \"learning_rate\": \"lr\",\n",
        "            \"per_gpu_train_batch_size\": \"train_bs/gpu\",\n",
        "            \"num_epochs\": \"num_epochs\"\n",
        "        },\n",
        "        metric_columns=[\n",
        "            \"eval_acc\", \"eval_loss\", \"epoch\", \"training_iteration\"\n",
        "        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnWPeLyWIb76"
      },
      "source": [
        "Finally, we pass in our training function, config, PBT scheduler, and reporter to tune:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy5X41IzIVd3"
      },
      "source": [
        "analysis = tune.run(\n",
        "        train_transformer,\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 1,\n",
        "            \"gpu\": 1\n",
        "        },\n",
        "        config=config,\n",
        "        num_samples=3,\n",
        "        scheduler=scheduler,\n",
        "        keep_checkpoints_num=3,\n",
        "        checkpoint_score_attr=\"training_iteration\",\n",
        "        progress_reporter=reporter,\n",
        "        local_dir=\"./ray_results/\",\n",
        "        name=\"tune_transformer_pbt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW2qMopVnMQY"
      },
      "source": [
        "Letâ€™s dive deeper into whatâ€™s going on here. Initially, tune creates 3 (from num_samples) trials, or instantiations of our training function. Each trial has a hyperparameter configuration provided by config. So we have 3 different executions of transformer fine-tuning, each with different hyperparameters, all running in parallel. However, we also pass in a PBT scheduler, with time_attr set to training_iteration and perturbation_interval set to 2. So, after 2 training iterations, we see PBT come into effect. The bottom 25% of trials according to eval_acc exploit from the top 25% of trials by copying over their model weights and hyperparameters. Then after copying over, we do exploration on these trials, by mutating certain hyperparameters specified by hyperparam_mutations. This is where checkpointing becomes crucial- this process results in a creation of a new trial, so we need checkpointing to continue training where we left off, except with the new hyperparameters. This process continues after each training iteration, and instead of randomly searching across our entire hyperparameter space, we can focus on the best performing trials and do a more fine-grained search in that smaller area."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSLqqV_CTzBj"
      },
      "source": [
        "## Testing the Best Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9yIl8LxT8a_"
      },
      "source": [
        "Once our hyperparameter tuning experiment is complete, we can get the best performin model and try it out on our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxKyvQ6WNlvG"
      },
      "source": [
        "data_args = DataTrainingArguments(task_name=config[\"task_name\"], data_dir=config[\"data_dir\"])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
        "\n",
        "best_config = analysis.get_best_config(metric=\"eval_acc\", mode=\"max\")\n",
        "print(best_config)\n",
        "best_checkpoint = recover_checkpoint(\n",
        "    analysis.get_best_trial(metric=\"eval_acc\",\n",
        "                            mode=\"max\").checkpoint.value)\n",
        "print(best_checkpoint)\n",
        "best_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    best_checkpoint).to(\"cuda\")\n",
        "\n",
        "test_args = TrainingArguments(output_dir=\"./best_model_results\", )\n",
        "test_dataset = GlueDataset(\n",
        "    data_args, tokenizer=tokenizer, mode=\"dev\", cache_dir=data_dir)\n",
        "test_dataset = test_dataset[len(test_dataset) // 2:]\n",
        "\n",
        "test_trainer = transformers.Trainer(\n",
        "    best_model,\n",
        "    test_args,\n",
        "    compute_metrics=utils.build_compute_metrics_fn(task_name))\n",
        "\n",
        "metrics = test_trainer.evaluate(test_dataset)\n",
        "print(metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-v_hzwTiVVt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}